---
title: MacroSheds Discharge and Chemistry Data Retrieval and Flux Calculation 
author: Weston Slaughter, Spencer Rhea, Mike Vlah
subtitle: Using the `macrosheds` package to retrieve discharge and chemistry data for target sites and variables, and to use it to calculate stream solute flux
output:
    html_document:
      toc: true 
      theme: united
      highlight: zenburn
      df_print: kable
---



This walkthrough is from the perspective of a first time user, with some knowledge of the R programming language. This example user, for a college project, needs to compare the flux of nitrate between two watersheds, one from the Eastern US and one from the Western US, each with at least 60 years of data.


# Install and explore the `macrosheds` package


To use the `macrosheds` package, you must first **make sure you have the `macrosheds` package on your computer, built, and installed in R**. Currently, the package is only available via github. If you have not already, you can install the `macrosheds` package as follows: 

```{r ms-install, eval = FALSE}
# install.packages('devtools') #you may need to install devtools too

devtools::install_github("https://github.com/MacroSHEDS/macrosheds")
```

Now, that we have the `macrosheds` package,  we load it into our library, along with other packages we will be using in this walkthrough. 

```{r setup-library, message = FALSE}
   
library(macrosheds)
# library(dplyr)
library(ggplot2)

```

In this walkthrough, to learn more about the `macrosheds` package, its functions, and how to use them, we will be using the `macrosheds` package's documentation. 

Before anything else, we need to know what functions are available. Let's explore the package with `help()`

```{r ms-help}
help(package = macrosheds)
```


# Identify target sites and variables in the MacroSheds dataset


Our first goal is to **identify sites we might want to pull data from**. To get more information about MacroSheds sites, we will use the function `download_ms_site_data`, which loads a tibble of MacroSheds site metadata. 

We will use `?` to get more information about the function, and how to use it.


#### ms\_download\_site\_data


```{r ms-site-data}
?ms_download_site_data
```

Looks like the function will run with no arugments, let's run it and look at the results.

```{r ms-sites}
ms_sites <- ms_download_site_data()
colnames(ms_sites)
```

Our objective is to find 2 watersheds, one in the Eastern US and one in the Western US, which both have at least 60 years of nitrate data.

The `ms_sites` dataframe columns include "latitude", "longitude", "first\_record\_utc", and "last\_record\_utc". Let's use these to see what sites meet our needs.


```{r ms-site-filter}
# calculate the amount of days in sixty years
sixty_yrs_days <- 365 * 60

# filter the dataframe of macrosheds sites
target_sites <- ms_sites %>%
    # to make sure our sites are roughly western or eastern 
    filter(longitude > -95 | longitude < -115) %>%
    mutate(period = as.numeric(
        difftime(last_record_utc,
                 first_record_utc,
                 units = "days"
                 ))) %>%
    # to get records > 60 years
    filter(period > sixty_yrs_days)

# let's list the unique sites left after our filtering
unique(target_sites$domain)
head(target_sites)
  
# and calculate summary statistics on these sites, to help
# choose which sites have the best data for our needs
target_summary <- target_sites %>%
    group_by(domain) %>%
    summarize(
        mean_long = mean(longitude),
        sum_obs = sum(n_observations),
        min_date = min(first_record_utc),
        max_date = min(last_record_utc)
    )
  
head(target_summary)
```

From our filter of the site metadata, we have found three domains which have site data that meets our requirements. By summarizing the data, we've found that two of these sites are in the Northeastern US (Fernow, HBEF) and one is on the West Coast (HJ Andrews). Our objective is to compare two sites, one east and one west, and between Fernow and HBEF our summary data show that HBEF has a greater number of overall observations, so let's go with that. 

```{r}
my_domains <- c("hbef", "hjandrews")
```

#### ms\_download\_variables

Now, we should choose the best site from HBEF and the best site from HJ Andrews -- in this case, which of each domain has the most complete Nitrate record.

To get data for our target variable, nitrate, let's make sure it's in the MacroSheds variable catalog, and see exactly what it is called. For this, we will use `ms_download_variables`, which also does not need arguments. 


```{r ms-vars}
ms_vars <- ms_catalog()
head(ms_vars)
```


This is a pretty long list, let's do a siple text search to see what variable names start with "Nitrate"

```{r ms-vars-filter}
ms_NO3 <- ms_vars[grep("^Nitrate", ms_vars$variable_name),]

# lets order the dataframe by number of obervations, and see whats at the top
ms_NO3 <- ms_NO3[order(-ms_NO3$observations),]
head(ms_NO3)

# Nitrate-N, with variable code NO3_N, tops the list
# let's make sure that our domains have robust records
m_NO3 <- ms_NO3 %>%
  filter(variable_code == "NO3_N") %>%
  filter(domain %in% my_domains)

head(ms_NO3)
```



# Retrieve discharge data for target sites


Now, we will retrieve the target data: discharge and nitrate for our two sites. First, we choose a directory to save the data to, and create a vector of target domain names. 

``` {r ms-dir}
my_ms_dir <- "./example/data"
```

We will also take a look at `ms_download_core_data` which we will sue to retrieve all the available data for our domains of interest.


##### ms\_download\_core\_data


```{r}
?ms_download_core_data
```

Now, we retrieve MacroSheds data by domain and save it to our directory.

``` {r ms-data, eval = FALSE}
options(timeout = 600) #default 60 might not be enough if your connection is slow

ms_download_core_data(
    my_ms_dir,
    domains = my_domains,
    quiet = FALSE
)
```


##### ms\_load\_product    *discharge*


Now we have the data downloaded, but we want to load specific variables into the R session. To do this, we use `ms_load_product`. 

```{r ms-load}
?ms_load_product
```

Pointing to the directory we downloaded the data to, we use this function to pull out discharge data for two sites.

```{r ms-q}
my_q <- ms_load_product(
    my_ms_dir,
    prodname = "discharge",
    site_codes = c("w1", "GSWS09"),
    sort_result = TRUE,
    warn = TRUE
)
```



# Retrieve stream chemistry data for target sites


#### ms\_load\_product    *chemistry*
Now that we have discharge, we will use `ms_load_product` again, but for nitrate-nitrogen data. As we know from searching the variable catalog, the code for nitrate-nitrogen is 'NO3_N'

```{r ms-chem}
my_chem <- ms_load_product(
    my_ms_dir,
    prodname = "stream_chemistry",
    filter_vars = "NO3_N",
    site_codes = c("w1", "GSWS09"),
    sort_result = TRUE,
    warn = TRUE
)

head(my_chem)
```

# Manipulate and clean data


#### ms\_synchronize\_timestep


Now that we have our nitrate-nitrogen data, we have to correct for differences in sampling frequency between the two sites. The MacroSheds package helps user achieve this via the `ms_synchronize_timestep` function.

```{r ms-sync, message = FALSE}
?ms_synchronize_timestep

my_chem_sync <- ms_synchronize_timestep(
    my_chem,
    desired_interval = "1 day",
    interpolate_limit = 40
)
```

# Calculate flux from this data


#### ms\_calc\_flux


Now that our nitrate-nitrogen data is pulled and synchronized, let's use `ms_calc_flux` to calculate nitrate-nitrogen flux from our discharge and concentration datasets. 

```{r}
?ms_calc_flux
```

We can plug in our discharge and chemistry data directly. 

```{r ms-flux}
my_flux <- ms_calc_flux(
    chemistry = my_chem_sync,
    q = my_q,
    q_type = 'discharge',
    site_info = ms_sites
)
head(my_flux)
```

From the documentation of `ms_calc_flux` we know, that our flux units are `kg/ha/T` where T is the sampling interval. This is ideal becuase we are comparing between two very different watersheds and it is appropriate to scale by area. But is you were intrested in clooking at the mass of a solute being exported from a watershed, not normalized to area, you can so with the MacroSheds function 'ms\_undo\_scale\_flux\_by\_area'.


#### ms\_scale\_flux\_by\_area


```{r ms-scale}
my_flux_unscaled <- ms_undo_scale_flux_by_area(my_flux)
```

Looks great! We have used the MacroSheds package and dataset to retrieve, synchronize, scale, and calculate flux from streamflow and nitrate-nitrogen concentration data from two sites on the east and west coasts of the United States with > 60 years of data. let's plot it!

```{r ms-explore}
my_annual_mean_flux <- my_flux %>%
    mutate(year = as.numeric(format(as.Date(datetime), format="%Y"))) %>%
    group_by(year, site_code) %>%
    summarize(
        mean_flux = sum(val),
        n = n(),
        .groups = 'drop',
    ) %>%
    filter(n > 330)

ggplot(data=my_annual_mean_flux, aes(x=year, y=mean_flux, group=site_code)) +
    geom_line(aes(color=site_code), lwd = 2)+
    geom_point() +
    scale_color_manual(
      values=c("#E69F00", "#56B4E9"),
      name = "Site"
    ) +
    ggtitle(
      label = "Mean Annual Nitrate-Nitrogen Flux",
      subtitle = "Hubbard Brook and HJ Andrews, 1970-2020"
    ) +
    xlab("Year") +
    ylab("Nitrate-Nitrogen Flux (kg/ha/day)") +
    theme_minimal() +
    theme(
      axis.text = element_text(size = 16),
      axis.title = element_text(size = 20),
      plot.title = element_text(size = 24),
      plot.subtitle = element_text(size = 22),
      legend.title = element_text(size = 21),
      legend.text = element_text(size = 20),
    ) 
  
```
